#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Vibe Coding åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨
æ•´åˆåˆ†æã€ç»Ÿè®¡ã€æ¡ˆä¾‹å±•ç¤ºåŠŸèƒ½
è¾“å‡ºï¼šé‡åŒ–æ•°æ® + çœŸå®æ¡ˆä¾‹ + åŸæ–‡å¼•ç”¨

è¾“å‡ºç›®å½•: analysis_report/
"""

import json
import csv
import os
import sys
import re
from collections import Counter, defaultdict
from datetime import datetime
from typing import List, Dict, Any, Optional

sys.stdout.reconfigure(encoding='utf-8')

# ========== é…ç½® ==========
INPUT_FILE = "vibe_coding_analysis_8cat.jsonl"  # ä½¿ç”¨8åˆ†ç±»ç‰ˆæœ¬çš„åˆ†æç»“æœ
OUTPUT_DIR = "analysis_report"
os.makedirs(OUTPUT_DIR, exist_ok=True)


def load_data() -> List[Dict]:
    """åŠ è½½åˆ†æç»“æœæ•°æ®"""
    results = []
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ {INPUT_FILE}")
        print("   è¯·å…ˆè¿è¡Œ deepseek_analyzer_8cat.py ç”Ÿæˆåˆ†æç»“æœ")
        return results
    
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                data = json.loads(line)
                results.append(data)
            except json.JSONDecodeError:
                continue
    
    print(f"âœ… å·²åŠ è½½ {len(results)} æ¡åˆ†æç»“æœ")
    return results


def save_csv(filename: str, headers: List[str], rows: List[List]):
    """ä¿å­˜CSVæ–‡ä»¶"""
    filepath = os.path.join(OUTPUT_DIR, filename)
    with open(filepath, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(headers)
        writer.writerows(rows)
    print(f"   ğŸ“„ CSV: {filepath}")


def analyze_by_macro_category(results: List[Dict]) -> Dict[str, Dict]:
    """æŒ‰å®è§‚åˆ†ç±»åˆ†æ"""
    macro_categories = defaultdict(list)
    
    for r in results:
        macro = r.get('macro_category', 'unknown')
        macro_categories[macro].append(r)
    
    analysis = {}
    for macro_name, repos in macro_categories.items():
        ai_scores = [r.get('ai_generation_score', 0) for r in repos]
        complexity_scores = [r.get('complexity_level', 0) for r in repos]
        stars_list = [r.get('stars', 0) for r in repos]
        
        # æ”¶é›†è¯¥å®è§‚åˆ†ç±»ä¸‹çš„å¾®è§‚åœºæ™¯åˆ†å¸ƒ
        micro_scenes = Counter([r.get('micro_scenario', 'unknown') for r in repos])
        
        analysis[macro_name] = {
            'count': len(repos),
            'percentage': len(repos) / len(results) * 100,
            'avg_ai_index': sum(ai_scores) / len(ai_scores) if ai_scores else 0,
            'avg_complexity': sum(complexity_scores) / len(complexity_scores) if complexity_scores else 0,
            'avg_stars': sum(stars_list) / len(stars_list) if stars_list else 0,
            'micro_distribution': dict(micro_scenes.most_common()),
        }
    
    return analysis


def generate_overview(results: List[Dict], macro_analysis: Dict) -> Dict:
    """ç”Ÿæˆæ€»ä½“æ¦‚è§ˆç»Ÿè®¡"""
    total = len(results)
    
    # åŸºç¡€ç»Ÿè®¡
    macro_cats = Counter([r.get('macro_category', 'unknown') for r in results])
    micro_scenes = Counter([r.get('micro_scenario', 'unknown') for r in results])
    ai_scores = [r.get('ai_generation_score', 0) for r in results]
    complexity_scores = [r.get('complexity_level', 0) for r in results]
    stars_list = [r.get('stars', 0) for r in results]
    
    # è®¡ç®—æŒ‡æ ‡
    high_ai_count = sum(1 for s in ai_scores if s >= 4)
    high_complexity_count = sum(1 for c in complexity_scores if c >= 4)
    high_star_count = sum(1 for s in stars_list if s >= 20)
    
    overview = {
        'total': total,
        'avg_ai_index': sum(ai_scores) / total if total > 0 else 0,
        'avg_complexity': sum(complexity_scores) / total if total > 0 else 0,
        'avg_stars': sum(stars_list) / total if total > 0 else 0,
        'high_ai_pct': high_ai_count / total * 100 if total > 0 else 0,
        'high_complexity_pct': high_complexity_count / total * 100 if total > 0 else 0,
        'high_star_pct': high_star_count / total * 100 if total > 0 else 0,
        'macro_distribution': dict(macro_cats.most_common()),
        'micro_distribution': dict(micro_scenes.most_common()),
        'macro_analysis': macro_analysis,
    }
    
    return overview


def analyze_by_category(results: List[Dict]) -> Dict[str, Dict]:
    """æŒ‰å¾®è§‚åœºæ™¯åˆ†ç±»åˆ†æ"""
    categories = defaultdict(list)
    
    for r in results:
        micro = r.get('micro_scenario', 'unknown')
        categories[micro].append(r)
    
    analysis = {}
    for cat_name, repos in categories.items():
        ai_scores = [r.get('ai_generation_score', 0) for r in repos]
        complexity_scores = [r.get('complexity_level', 0) for r in repos]
        stars_list = [r.get('stars', 0) for r in repos]
        
        # æ‰¾å…¸å‹æ¡ˆä¾‹ï¼ˆé«˜AIåˆ†æ•° + æœ‰æè¿°ï¼‰
        typical_cases = sorted(
            [r for r in repos if r.get('ai_generation_score', 0) >= 3 and r.get('core_intent')],
            key=lambda x: (x.get('ai_generation_score', 0), x.get('stars', 0)),
            reverse=True
        )[:5]  # æ¯ä¸ªåˆ†ç±»å–å‰5ä¸ªå…¸å‹æ¡ˆä¾‹
        
        # æ‰¾åŸæ–‡å¼•ç”¨ï¼ˆæœ‰æ´å¯Ÿçš„é¡¹ç›®ï¼‰
        insights = [r for r in repos if r.get('analytical_insight')]
        
        analysis[cat_name] = {
            'count': len(repos),
            'percentage': len(repos) / len(results) * 100,
            'avg_ai_index': sum(ai_scores) / len(ai_scores) if ai_scores else 0,
            'avg_complexity': sum(complexity_scores) / len(complexity_scores) if complexity_scores else 0,
            'avg_stars': sum(stars_list) / len(stars_list) if stars_list else 0,
            'typical_cases': typical_cases,
            'insights': insights[:3],  # å–3ä¸ªæ´å¯Ÿ
        }
    
    return analysis


def generate_markdown_report(results: List[Dict], overview: Dict, category_analysis: Dict):
    """ç”Ÿæˆ Markdown åˆ†ææŠ¥å‘Š"""
    report_path = os.path.join(OUTPUT_DIR, "vibe_coding_analysis_report.md")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        # æ ‡é¢˜
        f.write("# Vibe Coding èµ›é“åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n")
        f.write(f"**æ ·æœ¬è§„æ¨¡**: {overview['total']} ä¸ªé¡¹ç›®\n\n")
        
        # æ‰§è¡Œæ‘˜è¦
        f.write("---\n\n")
        f.write("## ğŸ“‹ æ‰§è¡Œæ‘˜è¦\n\n")
        f.write(f"- **å¹³å‡ AI ç‰¹å¾æŒ‡æ•°**: {overview['avg_ai_index']:.2f}/5.0\n")
        f.write(f"  - AIç‰¹å¾æŒ‡æ•°åæ˜ é¡¹ç›®ä¸­ä½“ç°å‡ºçš„AI codingå·¥å…·çš„ä½¿ç”¨ç‰¹å¾\n")
        f.write(f"  - 5åˆ†è¡¨ç¤ºå…·å¤‡æ˜æ˜¾çš„AIåŸç”Ÿç‰¹å¾ï¼ˆå¦‚AGENTS.mdã€.cursorrulesç­‰é…ç½®æ–‡ä»¶ï¼‰\n")
        f.write(f"- **é«˜ AI ç‰¹å¾é¡¹ç›® (â‰¥4åˆ†)**: {overview['high_ai_pct']:.1f}%\n")
        f.write(f"- **å¹³å‡å¤æ‚åº¦**: {overview['avg_complexity']:.2f}/5.0\n")
        f.write(f"- **å¹³å‡ Stars**: {overview['avg_stars']:.1f}\n\n")
        
        # å®è§‚åˆ†ç±»æ€»è¡¨
        f.write("---\n\n")
        f.write("## ğŸ—ï¸ å®è§‚åˆ†ç±»åˆ†å¸ƒæ€»è¡¨\n\n")
        f.write("| å®è§‚åˆ†ç±» | é¡¹ç›®æ•° | å æ¯” | å¹³å‡AIç‰¹å¾æŒ‡æ•° | å¹³å‡å¤æ‚åº¦ | å¹³å‡Stars |\n")
        f.write("|----------|--------|------|----------------|------------|-----------|\n")
        for cat_name, cat_data in overview['macro_analysis'].items():
            f.write(f"| {cat_name} | {cat_data['count']} | {cat_data['percentage']:.1f}% | "
                    f"{cat_data['avg_ai_index']:.2f} | {cat_data['avg_complexity']:.2f} | "
                    f"{cat_data['avg_stars']:.1f} |\n")
        
        # å®è§‚åˆ†ç±»è¯¦ç»†åˆ†æ
        f.write("\n### å®è§‚åˆ†ç±»è¯¦ç»†åˆ†æ\n\n")
        for macro_name, macro_data in overview['macro_analysis'].items():
            f.write(f"\n#### {macro_name}\n\n")
            f.write(f"**ç»Ÿè®¡æ¦‚è§ˆ**: {macro_data['count']}ä¸ªé¡¹ç›® ({macro_data['percentage']:.1f}%) | "
                    f"AIç‰¹å¾æŒ‡æ•° {macro_data['avg_ai_index']:.2f} | "
                    f"å¤æ‚åº¦ {macro_data['avg_complexity']:.2f} | "
                    f"Stars {macro_data['avg_stars']:.1f}\n\n")
            
            f.write("**å¾®è§‚åœºæ™¯åˆ†å¸ƒ**:\n\n")
            f.write("| å¾®è§‚åœºæ™¯ | æ•°é‡ | å æ¯” |\n")
            f.write("|----------|------|------|\n")
            for micro, count in list(macro_data['micro_distribution'].items())[:5]:
                micro_pct = count / macro_data['count'] * 100
                f.write(f"| {micro} | {count} | {micro_pct:.1f}% |\n")
        
        # å¾®è§‚åœºæ™¯è¯¦ç»†åˆ†æ
        f.write("\n---\n\n")
        f.write("## ğŸ¯ å¾®è§‚åœºæ™¯è¯¦ç»†åˆ†æ\n\n")
        
        # æŒ‰é¡¹ç›®æ•°é‡æ’åº
        sorted_cats = sorted(category_analysis.items(), key=lambda x: -x[1]['count'])
        
        for cat_name, cat_data in sorted_cats:
            f.write(f"\n### {cat_name}\n\n")
            f.write(f"**é¡¹ç›®æ•°é‡**: {cat_data['count']} ({cat_data['percentage']:.1f}%)\n\n")
            f.write(f"**å¹³å‡æŒ‡æ ‡**: AIç‰¹å¾æŒ‡æ•° {cat_data['avg_ai_index']:.2f} | ")
            f.write(f"å¤æ‚åº¦ {cat_data['avg_complexity']:.2f} | ")
            f.write(f"Stars {cat_data['avg_stars']:.1f}\n\n")
            
            # å…¸å‹æ¡ˆä¾‹
            if cat_data['typical_cases']:
                f.write("**å…¸å‹æ¡ˆä¾‹**:\n\n")
                for i, case in enumerate(cat_data['typical_cases'][:3], 1):
                    f.write(f"{i}. **{case.get('repo_name', 'N/A')}**\n")
                    f.write(f"   - AIç‰¹å¾æŒ‡æ•°: {'â˜…' * case.get('ai_generation_score', 0)}{'â˜†' * (5-case.get('ai_generation_score', 0))}\n")
                    f.write(f"   - æ ¸å¿ƒæ„å›¾: {case.get('core_intent', 'N/A')}\n")
                    f.write(f"   - å¤æ‚åº¦: {case.get('complexity_level', 0)}/5\n")
                    if case.get('description'):
                        f.write(f"   - é¡¹ç›®æè¿°: {case.get('description')[:100]}...\n")
                    f.write(f"   - åŸæ–‡æ´å¯Ÿ: *{case.get('analytical_insight', 'N/A')}*\n")
                    f.write(f"   - é“¾æ¥: {case.get('repo_url', 'N/A')}\n\n")
            
            # è¡Œä¸šæ´å¯Ÿå¼•ç”¨
            if cat_data['insights']:
                f.write("**è¡Œä¸šæ´å¯Ÿå¼•ç”¨**:\n\n")
                for insight_repo in cat_data['insights']:
                    f.write(f"> {insight_repo.get('analytical_insight', '')}\n")
                    f.write(f"> â€”â€” {insight_repo.get('repo_name')}\n\n")
        
        # é«˜ä»·å€¼é¡¹ç›® spotlightï¼ˆä»…è€ƒè™‘ Stars ç»´åº¦ï¼‰
        f.write("\n---\n\n")
        f.write("## â­ é«˜ä»·å€¼é¡¹ç›® Spotlight\n\n")
        f.write("> ç­›é€‰æ ‡å‡†: Stars â‰¥ 20\n\n")
        
        high_value = sorted(
            [r for r in results if r.get('stars', 0) >= 20],
            key=lambda x: x.get('stars', 0),
            reverse=True
        )[:10]
        
        for r in high_value:
            f.write(f"### {r.get('repo_name')}\n\n")
            f.write(f"- **Stars**: {r.get('stars', 0)} | **AIç‰¹å¾æŒ‡æ•°**: {r.get('ai_generation_score', 0)}/5\n")
            f.write(f"- **åˆ†ç±»**: {r.get('micro_scenario', 'N/A')}\n")
            f.write(f"- **æ ¸å¿ƒæ„å›¾**: {r.get('core_intent', 'N/A')}\n")
            if r.get('description'):
                f.write(f"- **æè¿°**: {r.get('description')[:150]}...\n")
            f.write(f"- **åŸæ–‡æ´å¯Ÿ**: {r.get('analytical_insight', 'N/A')}\n")
            f.write(f"- **é“¾æ¥**: {r.get('repo_url', 'N/A')}\n\n")
    
    print(f"âœ… Markdown æŠ¥å‘Š: {report_path}")
    return report_path


def generate_csv_reports(results: List[Dict], overview: Dict, category_analysis: Dict):
    """ç”Ÿæˆ CSV æ•°æ®æ–‡ä»¶"""
    
    # 0. å®è§‚åˆ†ç±»æ€»è¡¨
    rows = []
    for macro_name, macro_data in overview['macro_analysis'].items():
        rows.append([
            macro_name,
            macro_data['count'],
            f"{macro_data['percentage']:.1f}%",
            f"{macro_data['avg_ai_index']:.2f}",
            f"{macro_data['avg_complexity']:.2f}",
            f"{macro_data['avg_stars']:.1f}"
        ])
    save_csv("00_macro_category_overview.csv",
             ["å®è§‚åˆ†ç±»", "é¡¹ç›®æ•°", "å æ¯”", "å¹³å‡AIç‰¹å¾æŒ‡æ•°", "å¹³å‡å¤æ‚åº¦", "å¹³å‡Stars"],
             rows)
    
    # 1. æ€»ä½“åˆ†å¸ƒï¼ˆå¾®è§‚åœºæ™¯ï¼‰
    rows = []
    for scene, count in Counter([r.get('micro_scenario', 'unknown') for r in results]).most_common():
        cat_data = category_analysis.get(scene, {})
        rows.append([
            scene,
            count,
            f"{count/len(results)*100:.1f}%",
            f"{cat_data.get('avg_ai_index', 0):.2f}",
            f"{cat_data.get('avg_complexity', 0):.2f}",
            f"{cat_data.get('avg_stars', 0):.1f}"
        ])
    save_csv("01_micro_scenario_distribution.csv", 
             ["å¾®è§‚åœºæ™¯", "é¡¹ç›®æ•°", "å æ¯”", "å¹³å‡AIç‰¹å¾æŒ‡æ•°", "å¹³å‡å¤æ‚åº¦", "å¹³å‡Stars"], 
             rows)
    
    # 2. å®è§‚åˆ†ç±»è¯¦ç»†åˆ†å¸ƒ
    for macro_name, macro_data in overview['macro_analysis'].items():
        rows = []
        for micro, count in macro_data['micro_distribution'].items():
            micro_pct = count / macro_data['count'] * 100
            rows.append([micro, count, f"{micro_pct:.1f}%"])
        save_csv(f"macro_{macro_name.replace('/', '_')}_micro_distribution.csv",
                 ["å¾®è§‚åœºæ™¯", "æ•°é‡", "å æ¯”"],
                 rows)
    
    # 3. å…¸å‹æ¡ˆä¾‹è¯¦æƒ…
    rows = []
    for cat_name, cat_data in category_analysis.items():
        for case in cat_data.get('typical_cases', []):
            rows.append([
                cat_name,
                case.get('repo_name', ''),
                case.get('core_intent', ''),
                case.get('ai_generation_score', 0),
                case.get('complexity_level', 0),
                case.get('stars', 0),
                case.get('analytical_insight', '')[:100] + '...',
                case.get('repo_url', '')
            ])
    save_csv("03_typical_cases.csv",
             ["å¾®è§‚åœºæ™¯", "ä»“åº“å", "æ ¸å¿ƒæ„å›¾", "AIç‰¹å¾æŒ‡æ•°", "å¤æ‚åº¦", "Stars", "æ´å¯Ÿæ‘˜è¦", "é“¾æ¥"],
             rows)
    
    # 4. é«˜ä»·å€¼é¡¹ç›®ï¼ˆä»… Stars â‰¥ 20ï¼‰
    high_value = sorted(
        [r for r in results if r.get('stars', 0) >= 20],
        key=lambda x: x.get('stars', 0),
        reverse=True
    )
    rows = []
    for r in high_value:
        rows.append([
            r.get('repo_name', ''),
            r.get('micro_scenario', ''),
            r.get('stars', 0),
            r.get('ai_generation_score', 0),
            r.get('core_intent', ''),
            r.get('analytical_insight', '')[:150] + '...',
            r.get('repo_url', '')
        ])
    save_csv("04_high_value_projects.csv",
             ["ä»“åº“å", "å¾®è§‚åœºæ™¯", "Stars", "AIç‰¹å¾æŒ‡æ•°", "æ ¸å¿ƒæ„å›¾", "æ´å¯Ÿ", "é“¾æ¥"],
             rows)


def generate_insight_summary(results: List[Dict], category_analysis: Dict):
    """ç”Ÿæˆæ´å¯Ÿæ‘˜è¦æ–‡æœ¬"""
    summary_path = os.path.join(OUTPUT_DIR, "insights_summary.txt")
    
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("Vibe Coding èµ›é“æ ¸å¿ƒæ´å¯Ÿ\n")
        f.write("=" * 80 + "\n\n")
        
        total = len(results)
        f.write(f"ã€æ•°æ®æ¦‚è§ˆã€‘\n")
        f.write(f"æ ·æœ¬è§„æ¨¡: {total} ä¸ªé¡¹ç›®\n")
        f.write(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d')}\n\n")
        
        # Top 3 åœºæ™¯
        f.write("ã€ä¸»åŠ›å¾®è§‚åœºæ™¯ Top 3ã€‘\n\n")
        sorted_cats = sorted(category_analysis.items(), key=lambda x: -x[1]['count'])
        for i, (cat_name, cat_data) in enumerate(sorted_cats[:3], 1):
            f.write(f"{i}. {cat_name}\n")
            f.write(f"   é¡¹ç›®æ•°: {cat_data['count']} ({cat_data['percentage']:.1f}%)\n")
            f.write(f"   å¹³å‡AIç‰¹å¾æŒ‡æ•°: {cat_data['avg_ai_index']:.2f}/5\n")
            f.write(f"   å…¸å‹é¡¹ç›®:\n")
            for case in cat_data['typical_cases'][:2]:
                f.write(f"     - {case.get('repo_name')}: {case.get('core_intent')}\n")
            f.write("\n")
        
        # AI å‚ä¸è¶‹åŠ¿
        high_ai = sum(1 for r in results if r.get('ai_generation_score', 0) >= 4)
        f.write(f"ã€AI ç‰¹å¾æŒ‡æ•°æ´å¯Ÿã€‘\n")
        f.write(f"AIç‰¹å¾æŒ‡æ•°åæ˜ é¡¹ç›®ä¸­ä½“ç°å‡ºçš„AI codingå·¥å…·çš„ä½¿ç”¨ç‰¹å¾\n")
        f.write(f"5åˆ†è¡¨ç¤ºå…·å¤‡æ˜æ˜¾çš„AIåŸç”Ÿç‰¹å¾ï¼ˆå¦‚AGENTS.mdã€.cursorrulesç­‰é…ç½®æ–‡ä»¶ï¼‰\n")
        f.write(f"é«˜AIç‰¹å¾é¡¹ç›® (â‰¥4åˆ†): {high_ai} ä¸ª ({high_ai/total*100:.1f}%)\n")
        f.write(f"å¹³å‡AIç‰¹å¾æŒ‡æ•°: {sum(r.get('ai_generation_score', 0) for r in results)/total:.2f}/5\n\n")
        
        # å€¼å¾—å…³æ³¨çš„é¡¹ç›®
        f.write("ã€å€¼å¾—å…³æ³¨çš„é«˜ä»·å€¼é¡¹ç›®ã€‘\n\n")
        high_value = sorted(
            [r for r in results if r.get('stars', 0) >= 20 or r.get('ai_generation_score', 0) >= 4],
            key=lambda x: x.get('stars', 0),
            reverse=True
        )[:5]
        for r in high_value:
            f.write(f"- {r.get('repo_name')} ({r.get('stars')}â˜…, AIç‰¹å¾æŒ‡æ•°:{r.get('ai_generation_score')}/5)\n")
            f.write(f"  {r.get('core_intent')}\n")
            f.write(f"  æ´å¯Ÿ: {r.get('analytical_insight')}\n\n")
    
    print(f"âœ… æ´å¯Ÿæ‘˜è¦: {summary_path}")


def main():
    print("=" * 70)
    print("ğŸš€ Vibe Coding åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨")
    print("=" * 70)
    print(f"è¾“å…¥æ–‡ä»¶: {INPUT_FILE}")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}/\n")
    
    # åŠ è½½æ•°æ®
    results = load_data()
    if not results:
        print("âŒ æ²¡æœ‰æ•°æ®ï¼Œé€€å‡º")
        return
    
    # æŒ‰å®è§‚åˆ†ç±»åˆ†æ
    print("\nğŸ“Š æŒ‰å®è§‚åˆ†ç±»åˆ†æ...")
    macro_analysis = analyze_by_macro_category(results)
    
    # ç”Ÿæˆæ€»ä½“æ¦‚è§ˆ
    print("ğŸ“Š ç”Ÿæˆæ€»ä½“æ¦‚è§ˆ...")
    overview = generate_overview(results, macro_analysis)
    
    # æŒ‰å¾®è§‚åœºæ™¯åˆ†ç±»åˆ†æ
    print("ğŸ“Š æŒ‰å¾®è§‚åœºæ™¯åˆ†ç±»åˆ†æ...")
    category_analysis = analyze_by_category(results)
    
    # ç”Ÿæˆ Markdown æŠ¥å‘Š
    print("\nğŸ“ ç”Ÿæˆ Markdown æŠ¥å‘Š...")
    generate_markdown_report(results, overview, category_analysis)
    
    # ç”Ÿæˆ CSV æŠ¥å‘Š
    print("\nğŸ“„ ç”Ÿæˆ CSV æ•°æ®æ–‡ä»¶...")
    generate_csv_reports(results, overview, category_analysis)
    
    # ç”Ÿæˆæ´å¯Ÿæ‘˜è¦
    print("\nğŸ’¡ ç”Ÿæˆæ´å¯Ÿæ‘˜è¦...")
    generate_insight_summary(results, category_analysis)
    
    print("\n" + "=" * 70)
    print("âœ… åˆ†æå®Œæˆï¼")
    print(f"æ‰€æœ‰è¾“å‡ºä¿å­˜åœ¨: {OUTPUT_DIR}/")
    print("=" * 70)
    
    # æ‰“å°ç®€è¦ç»Ÿè®¡
    print(f"\nğŸ“ˆ ç®€è¦ç»Ÿè®¡:")
    print(f"   æ€»é¡¹ç›®æ•°: {overview['total']}")
    print(f"   å¹³å‡AIç‰¹å¾æŒ‡æ•°: {overview['avg_ai_index']:.2f}/5")
    print(f"   å¾®è§‚åœºæ™¯æ•°é‡: {len(category_analysis)}")
    print(f"   å®è§‚åˆ†ç±»æ•°é‡: {len(macro_analysis)}")
    print(f"\n   å¾®è§‚åœºæ™¯åˆ†å¸ƒ:")
    for cat, data in sorted(category_analysis.items(), key=lambda x: -x[1]['count'])[:5]:
        print(f"      {cat}: {data['count']} ({data['percentage']:.1f}%)")


if __name__ == "__main__":
    main()
