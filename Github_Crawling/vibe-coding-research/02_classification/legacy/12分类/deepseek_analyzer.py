#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Vibe Coding æ·±åº¦åˆ†æå™¨ - åŸºäº DeepSeek API

æ ¸å¿ƒç‰¹æ€§:
- æ–­ç‚¹ç»­ä¼ : è¿›åº¦è‡ªåŠ¨ä¿å­˜ï¼Œéšæ—¶ä¸­æ–­éšæ—¶æ¢å¤
- ç¨³å¥é‡è¯•: ç½‘ç»œæ³¢åŠ¨è‡ªåŠ¨é‡è¯•ï¼ŒæŒ‡æ•°é€€é¿
- å¹¶å‘æ§åˆ¶: æ”¯æŒå¹¶å‘è¯·æ±‚æå‡é€Ÿåº¦ï¼Œä½†æœ‰é€Ÿç‡ä¿æŠ¤
- æˆæœ¬æ§åˆ¶: å®æ—¶ç»Ÿè®¡ token ä½¿ç”¨ï¼Œæ”¯æŒé¢„ç®—ä¸Šé™
- åŒé‡è¾“å‡º: JSON + CSV ä¸¤ç§æ ¼å¼
- å®æ—¶ç»Ÿè®¡: ç»ˆç«¯æ˜¾ç¤ºè¿›åº¦ã€åˆ†ç±»åˆ†å¸ƒã€æˆæœ¬ä¼°ç®—
- ä¸¥æ ¼éµå¾ªæç¤ºè¯: å®Œå…¨ä½¿ç”¨ LLMæç¤ºè¯ æ–‡ä»¶çš„åˆ†ç±»é€»è¾‘

æ•°æ®è¯´æ˜:
- è¾“å…¥: vibe_coding_dataset_2w.jsonl (çº¦ 2103 ä¸ªä»“åº“)
- å®é™…å¾…åˆ†æ: çº¦ 1548 ä¸ªä»“åº“ (README éç©º)
- é¢„è®¡æˆæœ¬: çº¦ Â¥46 (æŒ‰ 0.03å…ƒ/æ¡è®¡ç®—)

ä½œè€…: AI Assistant
æ—¥æœŸ: 2026-02-11
"""

import os
import json
import csv
import time
import signal
import sys
import re
from datetime import datetime
from typing import Optional, Dict, Any, List, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from dataclasses import dataclass, asdict
from dotenv import load_dotenv
import requests

load_dotenv()
sys.stdout.reconfigure(encoding='utf-8')

# ========== é…ç½®å¸¸é‡ ==========
INPUT_FILE = "vibe_coding_dataset_2w.jsonl"
OUTPUT_JSON = "vibe_coding_analysis.jsonl"
OUTPUT_CSV = "vibe_coding_analysis.csv"
PROGRESS_FILE = "analyzer_progress.json"
FAILED_FILE = "analyzer_failed.jsonl"

# DeepSeek API é…ç½®
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"
DEEPSEEK_MODEL = "deepseek-chat"  # å¯é€‰: deepseek-chat, deepseek-reasoner

# å¹¶å‘å’Œé€Ÿç‡æ§åˆ¶
MAX_WORKERS = 5          # å¹¶å‘çº¿ç¨‹æ•°
REQUEST_DELAY = 0.5      # æ¯ä¸ªè¯·æ±‚é—´éš”(ç§’)
MAX_RETRIES = 3          # æœ€å¤§é‡è¯•æ¬¡æ•°
RETRY_DELAY_BASE = 2     # é‡è¯•åŸºç¡€å»¶è¿Ÿ(ç§’)

# æˆæœ¬æ§åˆ¶
MAX_BUDGET_CNY = 50      # é¢„ç®—ä¸Šé™(äººæ°‘å¸å…ƒ)ï¼Œçº¦ç­‰äº 500ä¸‡ tokens
# DeepSeek chat æ¨¡å‹: è¾“å…¥ 1å…ƒ/ç™¾ä¸‡token, è¾“å‡º 2å…ƒ/ç™¾ä¸‡token
# å¹³å‡æ¯ä¸ªè¯·æ±‚çº¦ 2000 è¾“å…¥ + 500 è¾“å‡º = 3åˆ†é’±

# è¯»å–æç¤ºè¯æ–‡ä»¶
PROMPT_FILE = "LLMæç¤ºè¯"


@dataclass
class AnalysisResult:
    """åˆ†æç»“æœæ•°æ®ç»“æ„"""
    # è¾“å…¥æ•°æ®
    repo_id: int
    repo_name: str
    repo_url: str
    stars: int
    description: Optional[str]
    language: Optional[str]
    topics: List[str]
    tier: str
    size_kb: int
    
    # åˆ†æç»“æœ
    ai_generation_score: int
    core_intent: str
    macro_category: str
    micro_scenario: str
    complexity_level: int
    analytical_insight: str
    
    # å…ƒæ•°æ®
    analyzed_at: str
    tokens_input: int = 0
    tokens_output: int = 0
    api_cost_cny: float = 0.0
    retry_count: int = 0


class VibeCodingAnalyzer:
    """Vibe Coding åˆ†æå™¨ä¸»ç±»"""
    
    def __init__(self):
        self.api_key = DEEPSEEK_API_KEY
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total": 0,              # æ€»ä»“åº“æ•°
            "readme_ok": 0,          # README æœ‰æ•ˆçš„
            "processed": 0,          # å·²å¤„ç†
            "success": 0,            # æˆåŠŸ
            "failed": 0,             # å¤±è´¥
            "skipped": 0,            # è·³è¿‡(å·²å¤„ç†è¿‡)
            "tokens_input": 0,       # æ€»è¾“å…¥ token
            "tokens_output": 0,      # æ€»è¾“å‡º token
            "total_cost_cny": 0.0,   # æ€»æˆæœ¬
            "start_time": None,
            "end_time": None,
        }
        
        # åˆ†ç±»åˆ†å¸ƒç»Ÿè®¡
        self.category_stats = {
            "macro_category": {},
            "micro_scenario": {},
            "ai_generation_score": {i: 0 for i in range(1, 6)},
            "complexity_level": {i: 0 for i in range(1, 6)},
        }
        
        # è¿›åº¦è·Ÿè¸ª
        self.processed_ids: Set[int] = set()
        self.failed_ids: Set[int] = set()
        self.lock = Lock()
        self.running = True
        
        # åŠ è½½ç³»ç»Ÿæç¤ºè¯
        self.system_prompt = self._load_system_prompt()
    
    def _load_system_prompt(self) -> str:
        """åŠ è½½æç¤ºè¯æ–‡ä»¶"""
        try:
            if os.path.exists(PROMPT_FILE):
                with open(PROMPT_FILE, 'r', encoding='utf-8') as f:
                    return f.read()
            else:
                print(f"âš ï¸ æç¤ºè¯æ–‡ä»¶ {PROMPT_FILE} ä¸å­˜åœ¨ï¼Œä½¿ç”¨å†…ç½®æç¤ºè¯")
                return self._get_default_prompt()
        except Exception as e:
            print(f"âš ï¸ è¯»å–æç¤ºè¯æ–‡ä»¶å¤±è´¥: {e}")
            return self._get_default_prompt()
    
    def _get_default_prompt(self) -> str:
        """ä½¿ç”¨ LLMæç¤ºè¯ æ–‡ä»¶ä¸­çš„å®Œæ•´å†…å®¹"""
        return """ä½ æ˜¯ä¸€ä½æ·±è°™ç§‘æŠ€è¡Œä¸šä¸é£é™©æŠ•èµ„è¶‹åŠ¿çš„æ•°æ®åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æ GitHub ä»“åº“çš„å…ƒæ•°æ®å’Œ README å†…å®¹ï¼Œä»ä¸­æç‚¼å‡ºå¼€å‘è€…çš„çœŸå®æ„å»ºæ„å›¾ã€è¡Œä¸šè½åœ°åœºæ™¯ä»¥åŠ AI å‚ä¸ç¼–ç¨‹çš„æµ“åº¦ã€‚

è¯·é˜…è¯»æä¾›çš„å•ä¸ª GitHub é¡¹ç›®æ•°æ®ï¼ˆJSON æ ¼å¼ï¼‰ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON ç»“æ„è¾“å‡ºä½ çš„åˆ†æç»“æœã€‚å¿…é¡»è¾“å‡ºåˆæ³•çš„ JSON æ ¼å¼ï¼Œç¦æ­¢åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šæ€§æ–‡æœ¬ã€‚

åˆ†æç»´åº¦ä¸è¾“å‡ºå­—æ®µè¯´æ˜ï¼š

1. ai_generation_score (æ•´æ•° 1 åˆ° 5)
è¯„ä¼°è¯¥é¡¹ç›®å€ŸåŠ© AI è¾…åŠ©ç”Ÿæˆï¼ˆVibe Codingï¼‰çš„ç¨‹åº¦ã€‚æ³¨æ„ï¼šç°ä»£ AI å®Œå…¨å¯ä»¥ç”Ÿæˆå¤æ‚çš„å·¥ç¨‹æ¶æ„ï¼Œå› æ­¤ä¸èƒ½å•çº¯æŒ‰ä»£ç å¤æ‚åº¦æ‰“åˆ†ï¼Œè€Œåº”å¯»æ‰¾ AI å·¥ä½œæµçš„ç‰¹å¾ã€‚
- 1: æä½ã€‚æ˜ç¡®çš„ä¼ ç»Ÿäººå·¥æ‰‹å†™ç—•è¿¹ï¼Œç¼ºä¹ä»»ä½• AI é…ç½®æ–‡ä»¶ï¼Œå¸¸è§„çš„ç»†ç²’åº¦å¼€å‘è®°å½•ã€‚
- 3: ä¸­ç­‰ã€‚æ··åˆå¼€å‘ï¼Œå¯èƒ½ä½¿ç”¨äº† AI è¾…åŠ©è¡¥å…¨ä»£ç ï¼Œä½†åœ¨ç³»ç»Ÿè®¾è®¡å’Œ README ç¼–å†™ä¸Šä¿ç•™äº†ä¸ªäººå®šåˆ¶åŒ–çš„äººç±»ç—•è¿¹ã€‚
- 5: æé«˜ã€‚å…·å¤‡æ˜æ˜¾çš„ AI åŸç”Ÿç‰¹å¾ï¼Œä¾‹å¦‚ï¼šæ ¹ç›®å½•åŒ…å«ç‰¹å®šçš„æ¨¡å‹ä¸Šä¸‹æ–‡åè®®æ–‡ä»¶ï¼ˆå¦‚ AGENTS.md, .cursorrules, CLAUDE.md, .devin, .windsurf, .mcp.json, .aider.conf.yml, .github/copilot-instructions.mdï¼‰ï¼›æˆ–æ˜¯ç›´æ¥ä» Google/Vercel ç­‰å®˜æ–¹ AI æ¨¡æ¿ç”Ÿæˆçš„é¡¹ç›®ï¼›æˆ–æ˜¯ README å…·æœ‰é«˜åº¦æ ‡å‡†åŒ–çš„æœºå™¨ç”Ÿæˆè¯­æ„Ÿã€‚

2. core_intent (å­—ç¬¦ä¸²)
ç”¨ä¸€å¥è¯ï¼ˆä¸è¶…è¿‡ 15 ä¸ªå­—ï¼‰æå…¶ç²¾ç‚¼åœ°æ¦‚æ‹¬è¯¥é¡¹ç›®è§£å†³çš„æ ¸å¿ƒç—›ç‚¹æˆ–ä¸šåŠ¡é€»è¾‘ã€‚æŠ›å¼ƒè¥é”€è¯æ±‡ï¼Œç›´å‡»æœ¬è´¨ã€‚ä¾‹å¦‚ï¼šæŠ“å–æŒ‡å®šç½‘é¡µå¹¶æ¨é€åˆ°é£ä¹¦ã€å°†PDFè½¬æ¢ä¸ºæ’­å®¢ã€‚

3. macro_category (å­—ç¬¦ä¸²)
è¯„ä¼°é¡¹ç›®å½¢æ€çš„å®è§‚ç»´åº¦ã€‚å¿…é¡»ä»ä»¥ä¸‹ 3 ä¸ªé€‰é¡¹ä¸­ç²¾ç¡®é€‰æ‹©å…¶ä¸€ï¼š
- ä¸ªäººæ•ˆèƒ½ä¸è¾…åŠ©å·¥å…·ï¼šæœåŠ¡äºå•ä¸€ç”¨æˆ·çš„æ—¥å¸¸ç—›ç‚¹ï¼Œé€šå¸¸æ˜¯è„šæœ¬æˆ–è‡ªåŠ¨åŒ–æµç¨‹ï¼Œä»¥æå‡ä¸ªäººç”Ÿäº§åŠ›ä¸ºæ ¸å¿ƒç›®çš„ã€‚åå‘è½»é‡çº§çš„ææ•ˆã€‚
- åŸºç¡€è®¾æ–½ä¸åº•å±‚ç»„ä»¶ï¼šæœåŠ¡äºå…¶ä»–è½¯ä»¶æˆ–ç³»ç»Ÿå¼€å‘ï¼Œæ³›æŒ‡å„ç±»åŒ…ã€æ¡†æ¶ã€åè®®ã€ä¸­é—´ä»¶æˆ–æ•°æ®åº“è¿æ¥å™¨ï¼Œä¸ç›´æ¥é¢å‘ç»ˆç«¯éæŠ€æœ¯ç”¨æˆ·ã€‚
- äº§å“ä¸ç³»ç»ŸåŸå‹ï¼šæœåŠ¡äºå¤šç”¨æˆ·ç¾¤ä½“ï¼Œå…·å¤‡å®Œæ•´çš„å‰åç«¯ç»“æ„æˆ–äº¤äº’ç•Œé¢ï¼Œå¸¦æœ‰äº§å“éªŒè¯æ€§è´¨ã€‚

4. micro_scenario (å­—ç¬¦ä¸²)
è¯„ä¼°é¡¹ç›®ä¸šåŠ¡åœºæ™¯çš„å¾®è§‚ç»´åº¦ã€‚å¿…é¡»ä»ä»¥ä¸‹ 12 ä¸ªé€‰é¡¹ä¸­ç²¾ç¡®é€‰æ‹©å…¶ä¸€ï¼Œé€‰æ‹©æœ€æ ¸å¿ƒçš„è½åœ°åœºæ™¯ï¼š
- productivity: è·¨è¡Œä¸šçš„é€šç”¨æ•ˆç‡æå‡ï¼Œå¦‚ç¬”è®°ç®¡ç†ã€æ—¥ç¨‹è§„åˆ’ã€é€šç”¨æ ¼å¼è½¬æ¢ç­‰ã€‚
- content_creation: ä¸“é—¨é’ˆå¯¹å›¾ã€æ–‡ã€éŸ³ã€è§†ç­‰åª’ä½“å½¢æ€çš„ç”Ÿæˆã€ç¼–è¾‘ä¸å¤„ç†ã€‚
- business_automation: ä¼ä¸šçº§æˆ–ç‰¹å®šå•†ä¸šæµç¨‹çš„è‡ªåŠ¨åŒ–ï¼Œå¦‚ CRM ç®¡ç†ã€é”€å”®æ¼æ–—ã€å®¢æœè‡ªåŠ¨å›å¤ç­‰ã€‚
- education: çŸ¥è¯†ä¼ æˆã€è€ƒè¯•è¾…åŠ©ä¸æŠ€èƒ½å­¦ä¹ åœºæ™¯ã€‚
- social: äººä¸äººä¹‹é—´çš„ä¿¡æ¯äº¤æ¢ã€é€šè®¯åŒ¹é…ä¸ç¤¾åŒºè¿æ¥ã€‚
- fintech: èµ„é‡‘æµè½¬ã€åŠ å¯†èµ„äº§äº¤æ˜“ã€é‡åŒ–è„šæœ¬ä¸é‡‘èæ•°æ®åˆ†æã€‚
- health: åŒ»ç–—æ•°æ®è§£æã€å¥åº·è¿½è¸ªã€é¥®é£Ÿä¸è¿åŠ¨ç®¡ç†ã€‚
- entertainment: æ¸¸æˆã€äº’åŠ¨å°è¯´ä¸çº¯å¨±ä¹æ¶ˆè´¹åœºæ™¯ã€‚
- research: å­¦æœ¯å®éªŒã€æ•°æ®ç§‘å­¦æ¢ç´¢ã€çˆ¬è™«é‡‡é›†ä¸éå•†ä¸šåŒ–å‰æ²¿ç ”ç©¶ã€‚
- personal: å®¶åº­ä¸ä¸ªäººç”Ÿæ´»ç®¡ç†ï¼Œå¦‚èœè°±èšåˆã€ç§äººè®°è´¦ã€æ™ºèƒ½å®¶å±…ç‰©è”ç½‘æ§åˆ¶ã€‚
- ecommerce: å•†å“ä¹°å–ã€åº“å­˜ç®¡ç†ã€æŠ¢ç¥¨è„šæœ¬ä¸ä»·æ ¼åŠ¨æ€ç›‘æµ‹ã€‚
- other: ç¡®å®æ— æ³•å½’å…¥ä¸Šè¿°ä»»ä½•ä¸šåŠ¡åœºæ™¯çš„æå…¶ç½•è§çš„è¾¹ç•Œæ¡ˆä¾‹ã€‚

5. complexity_level (æ•´æ•° 1 åˆ° 5)
è¯„ä¼°è¯¥é¡¹ç›®çš„ä¸šåŠ¡é€»è¾‘å¤æ‚ç¨‹åº¦ã€‚1 ä»£è¡¨æå…¶ç®€å•çš„å•æ–‡ä»¶è„šæœ¬æˆ–çº¯æ–‡æœ¬é…ç½®ï¼Œ5 ä»£è¡¨æ¶‰åŠå¤šæ–¹ API æœåŠ¡è°ƒç”¨å¹¶å…·å¤‡å®Œæ•´çŠ¶æ€ç®¡ç†å’ŒæŒä¹…åŒ–å­˜å‚¨çš„å¤æ‚ç³»ç»Ÿã€‚

6. analytical_insight (å­—ç¬¦ä¸²)
ç«™åœ¨è¡Œä¸šåˆ†æå¸ˆçš„è§’åº¦ï¼Œç”¨ç®€çŸ­çš„ä¸€ä¸¤å¥è¯è¯„ä»·è¿™ä¸ªé¡¹ç›®åæ˜ äº†å½“ä¸‹è½¯ä»¶å¼€å‘ç”Ÿæ€ä¸­çš„å“ªç§å¾®è§‚è¶‹åŠ¿ã€‚

å¿…é¡»è¾“å‡ºåˆæ³•çš„ JSON æ ¼å¼ï¼Œç¦æ­¢åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šæ€§æ–‡æœ¬ã€‚"""

    def load_progress(self) -> None:
        """åŠ è½½è¿›åº¦ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰"""
        if not os.path.exists(PROGRESS_FILE):
            return
        
        try:
            with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.processed_ids = set(data.get('processed_ids', []))
                self.failed_ids = set(data.get('failed_ids', []))
                self.stats['processed'] = len(self.processed_ids)
                self.stats['failed'] = len(self.failed_ids)
            print(f"ğŸ“‚ å·²åŠ è½½è¿›åº¦: {len(self.processed_ids)} ä¸ªå·²å¤„ç†, {len(self.failed_ids)} ä¸ªå¤±è´¥")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½è¿›åº¦å¤±è´¥: {e}")
    
    def save_progress(self) -> None:
        """ä¿å­˜è¿›åº¦"""
        data = {
            'processed_ids': list(self.processed_ids),
            'failed_ids': list(self.failed_ids),
            'stats': self.stats,
            'category_stats': self.category_stats,
            'saved_at': datetime.now().isoformat()
        }
        try:
            with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    def load_repos(self) -> List[Dict]:
        """åŠ è½½ä»“åº“æ•°æ®ï¼Œåªè¿”å› readme ä¸ä¸º null ä¸”æœªå¤„ç†è¿‡çš„"""
        repos = []
        readme_null_count = 0
        already_processed = 0
        
        try:
            with open(INPUT_FILE, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        repo = json.loads(line)
                        repo_id = repo.get('id')
                        
                        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
                        if repo_id in self.processed_ids:
                            already_processed += 1
                            continue
                        
                        # æ£€æŸ¥ README
                        readme = repo.get('readme_content')
                        if readme is None or readme == '':
                            readme_null_count += 1
                            continue
                        
                        repos.append(repo)
                    except json.JSONDecodeError:
                        continue
        except FileNotFoundError:
            print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ {INPUT_FILE} ä¸å­˜åœ¨")
            return []
        
        self.stats['total'] = len(repos) + already_processed + readme_null_count
        self.stats['readme_ok'] = len(repos) + already_processed
        
        print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"   æ€»ä»“åº“æ•°: {self.stats['total']}")
        print(f"   READMEæœ‰æ•ˆ: {self.stats['readme_ok']} ({readme_null_count} ä¸ªä¸ºç©º)")
        print(f"   å¾…å¤„ç†: {len(repos)} ({already_processed} ä¸ªå·²å¤„ç†å°†è·³è¿‡)")
        
        return repos
    
    def _build_prompt(self, repo: Dict) -> str:
        """æ„å»ºåˆ†ææç¤ºè¯ - ä¸¥æ ¼æŒ‰ç…§ LLMæç¤ºè¯ æ–‡ä»¶è¦æ±‚"""
        # README æˆªæ–­ç­–ç•¥ï¼šä¿ç•™å‰ 10000 å­—ç¬¦ï¼ˆçº¦ 2500 tokensï¼‰ï¼Œè¶³å¤Ÿåˆ¤æ–­ AI ç‰¹å¾
        readme = repo.get('readme_content', '')
        max_readme_len = 10000
        if len(readme) > max_readme_len:
            readme = readme[:max_readme_len] + "\n\n[README å·²æˆªæ–­...]"
        
        # æ„å»ºå®Œæ•´çš„ä»“åº“æ•°æ® JSON
        repo_data = {
            "repo_name": repo.get('repo_name'),
            "repo_url": repo.get('repo_url'),
            "stars": repo.get('stars'),
            "description": repo.get('description'),
            "language": repo.get('language'),
            "topics": repo.get('topics', []),
            "tier": repo.get('tier'),
            "size_kb": repo.get('size_kb'),
            "forks_count": repo.get('forks_count'),
            "open_issues": repo.get('open_issues'),
            "created_at": repo.get('created_at'),
            "pushed_at": repo.get('pushed_at'),
            "readme_content": readme,
        }
        
        return f"è¯·åˆ†æä»¥ä¸‹ GitHub ä»“åº“çš„å…ƒæ•°æ®å’Œ README å†…å®¹ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§æŒ‡å®š JSON ç»“æ„è¾“å‡ºåˆ†æç»“æœã€‚\n\nã€ä»“åº“æ•°æ®ã€‘\n```json\n{json.dumps(repo_data, ensure_ascii=False, indent=2)}\n```\n\nã€è¦æ±‚ã€‘\n1. å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ç³»ç»Ÿæç¤ºè¯ä¸­çš„ 6 ä¸ªç»´åº¦è¿›è¡Œåˆ†æ\n2. ai_generation_score: 1-5 æ•´æ•°ï¼Œå¯»æ‰¾ AI å·¥ä½œæµç‰¹å¾ï¼ˆAGENTS.md, .cursorrules ç­‰ï¼‰\n3. core_intent: ä¸€å¥è¯ï¼ˆâ‰¤15å­—ï¼‰ï¼Œç›´å‡»æœ¬è´¨ï¼ŒæŠ›å¼ƒè¥é”€è¯æ±‡\n4. macro_category: ä¸‰é€‰ä¸€ï¼ˆä¸ªäººæ•ˆèƒ½ä¸è¾…åŠ©å·¥å…· / åŸºç¡€è®¾æ–½ä¸åº•å±‚ç»„ä»¶ / äº§å“ä¸ç³»ç»ŸåŸå‹ï¼‰\n5. micro_scenario: åäºŒé€‰ä¸€ï¼ˆproductivity/content_creation/business_automation/education/social/fintech/health/entertainment/research/personal/ecommerce/otherï¼‰\n6. complexity_level: 1-5 æ•´æ•°ï¼Œè¯„ä¼°ä¸šåŠ¡é€»è¾‘å¤æ‚åº¦\n7. analytical_insight: ç®€çŸ­ä¸€ä¸¤å¥è¯çš„è¡Œä¸šè¶‹åŠ¿æ´å¯Ÿ\n\nåªè¾“å‡ºåˆæ³• JSONï¼Œç¦æ­¢ä»»ä½•é¢å¤–è§£é‡Šã€‚"
    
    def _parse_response(self, content: str) -> Optional[Dict]:
        """è§£æ API å“åº”"""
        try:
            # æ¸…ç† markdown ä»£ç å—
            content = content.strip()
            if content.startswith("```json"):
                content = content[7:]
            if content.startswith("```"):
                content = content[3:]
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()
            
            # å°è¯•ç›´æ¥è§£æ
            result = json.loads(content)
            
            # éªŒè¯å¿…è¦å­—æ®µ
            required_fields = [
                'ai_generation_score', 'core_intent', 'macro_category',
                'micro_scenario', 'complexity_level', 'analytical_insight'
            ]
            for field in required_fields:
                if field not in result:
                    print(f"    âš ï¸ ç¼ºå°‘å­—æ®µ: {field}")
                    return None
            
            # éªŒè¯æ•°å€¼èŒƒå›´
            if not (1 <= result['ai_generation_score'] <= 5):
                result['ai_generation_score'] = max(1, min(5, result.get('ai_generation_score', 3)))
            if not (1 <= result['complexity_level'] <= 5):
                result['complexity_level'] = max(1, min(5, result.get('complexity_level', 3)))
            
            # éªŒè¯æšä¸¾å€¼
            valid_macro = ["ä¸ªäººæ•ˆèƒ½ä¸è¾…åŠ©å·¥å…·", "åŸºç¡€è®¾æ–½ä¸åº•å±‚ç»„ä»¶", "äº§å“ä¸ç³»ç»ŸåŸå‹"]
            if result['macro_category'] not in valid_macro:
                result['macro_category'] = "äº§å“ä¸ç³»ç»ŸåŸå‹"  # é»˜è®¤
            
            valid_micro = [
                "productivity", "content_creation", "business_automation",
                "education", "social", "fintech", "health", "entertainment",
                "research", "personal", "ecommerce", "other"
            ]
            if result['micro_scenario'] not in valid_micro:
                result['micro_scenario'] = "other"
            
            return result
            
        except json.JSONDecodeError as e:
            print(f"    âš ï¸ JSON è§£æå¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"    âš ï¸ è§£æå¼‚å¸¸: {e}")
            return None
    
    def analyze_single(self, repo: Dict) -> Optional[AnalysisResult]:
        """åˆ†æå•ä¸ªä»“åº“"""
        repo_id = repo.get('id')
        repo_name = repo.get('repo_name', 'unknown')
        
        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
        if repo_id in self.processed_ids:
            return None
        
        prompt = self._build_prompt(repo)
        
        payload = {
            "model": DEEPSEEK_MODEL,
            "messages": [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.3,
            "max_tokens": 600,
            "response_format": {"type": "json_object"}
        }
        
        retry_count = 0
        last_error = None
        
        for attempt in range(MAX_RETRIES):
            if not self.running:
                return None
            
            try:
                response = requests.post(
                    DEEPSEEK_API_URL,
                    headers=self.headers,
                    json=payload,
                    timeout=60
                )
                response.raise_for_status()
                
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                
                # è·å– token ä½¿ç”¨é‡
                usage = result.get("usage", {})
                tokens_input = usage.get("prompt_tokens", 0)
                tokens_output = usage.get("completion_tokens", 0)
                
                # è®¡ç®—æˆæœ¬ (DeepSeek Chat: è¾“å…¥ 1å…ƒ/M, è¾“å‡º 2å…ƒ/M)
                cost = (tokens_input * 1 + tokens_output * 2) / 1_000_000
                
                # è§£æç»“æœ
                parsed = self._parse_response(content)
                if parsed is None:
                    retry_count += 1
                    if attempt < MAX_RETRIES - 1:
                        time.sleep(RETRY_DELAY_BASE * (2 ** attempt))
                        continue
                    return None
                
                # æ„å»ºç»“æœå¯¹è±¡
                analysis = AnalysisResult(
                    repo_id=repo_id,
                    repo_name=repo_name,
                    repo_url=repo.get('repo_url', ''),
                    stars=repo.get('stars', 0),
                    description=repo.get('description'),
                    language=repo.get('language'),
                    topics=repo.get('topics', []),
                    tier=repo.get('tier', ''),
                    size_kb=repo.get('size_kb', 0),
                    ai_generation_score=parsed['ai_generation_score'],
                    core_intent=parsed['core_intent'],
                    macro_category=parsed['macro_category'],
                    micro_scenario=parsed['micro_scenario'],
                    complexity_level=parsed['complexity_level'],
                    analytical_insight=parsed['analytical_insight'],
                    analyzed_at=datetime.now().isoformat(),
                    tokens_input=tokens_input,
                    tokens_output=tokens_output,
                    api_cost_cny=cost,
                    retry_count=retry_count
                )
                
                return analysis
                
            except requests.exceptions.RequestException as e:
                last_error = str(e)
                retry_count += 1
                if attempt < MAX_RETRIES - 1:
                    sleep_time = RETRY_DELAY_BASE * (2 ** attempt)
                    print(f"    ğŸ”„ è¯·æ±‚å¤±è´¥ï¼Œ{sleep_time}ç§’åé‡è¯• ({attempt+1}/{MAX_RETRIES}): {e}")
                    time.sleep(sleep_time)
                continue
            except Exception as e:
                last_error = str(e)
                retry_count += 1
                if attempt < MAX_RETRIES - 1:
                    time.sleep(RETRY_DELAY_BASE)
                continue
        
        # å…¨éƒ¨é‡è¯•å¤±è´¥
        print(f"    âŒ æœ€ç»ˆå¤±è´¥: {last_error}")
        self._save_failed(repo, last_error)
        return None
    
    def _save_failed(self, repo: Dict, error: str) -> None:
        """ä¿å­˜å¤±è´¥çš„è®°å½•"""
        with self.lock:
            self.failed_ids.add(repo.get('id'))
        
        failed_data = {
            'repo_id': repo.get('id'),
            'repo_name': repo.get('repo_name'),
            'error': error,
            'failed_at': datetime.now().isoformat()
        }
        with open(FAILED_FILE, 'a', encoding='utf-8') as f:
            json.dump(failed_data, f, ensure_ascii=False)
            f.write('\n')
    
    def _save_result(self, result: AnalysisResult) -> None:
        """ä¿å­˜å•ä¸ªç»“æœ"""
        # ä¿å­˜åˆ° JSONL
        with open(OUTPUT_JSON, 'a', encoding='utf-8') as f:
            json.dump(asdict(result), f, ensure_ascii=False)
            f.write('\n')
        
        # ä¿å­˜åˆ° CSV (è¿½åŠ æ¨¡å¼)
        file_exists = os.path.exists(OUTPUT_CSV)
        with open(OUTPUT_CSV, 'a', encoding='utf-8-sig', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=asdict(result).keys())
            if not file_exists:
                writer.writeheader()
            writer.writerow(asdict(result))
        
        # æ›´æ–°ç»Ÿè®¡
        with self.lock:
            self.processed_ids.add(result.repo_id)
            self.stats['success'] += 1
            self.stats['tokens_input'] += result.tokens_input
            self.stats['tokens_output'] += result.tokens_output
            self.stats['total_cost_cny'] += result.api_cost_cny
            
            # æ›´æ–°åˆ†ç±»ç»Ÿè®¡
            self.category_stats['macro_category'][result.macro_category] = \
                self.category_stats['macro_category'].get(result.macro_category, 0) + 1
            self.category_stats['micro_scenario'][result.micro_scenario] = \
                self.category_stats['micro_scenario'].get(result.micro_scenario, 0) + 1
            self.category_stats['ai_generation_score'][result.ai_generation_score] += 1
            self.category_stats['complexity_level'][result.complexity_level] += 1
    
    def _print_progress(self) -> None:
        """æ‰“å°è¿›åº¦ä¿¡æ¯"""
        s = self.stats
        total_to_process = s['readme_ok'] - s['processed'] + s['success'] + s['failed']
        progress = (s['success'] + s['failed']) / total_to_process * 100 if total_to_process > 0 else 0
        
        elapsed = time.time() - s['start_time']
        rate = s['success'] / elapsed * 60 if elapsed > 0 else 0  # æ¯åˆ†é’Ÿå¤„ç†æ•°
        
        remaining = total_to_process - s['success'] - s['failed']
        eta_seconds = remaining / (s['success'] / elapsed) if s['success'] > 0 and elapsed > 0 else 0
        eta_minutes = int(eta_seconds / 60)
        
        # æ¸…å±å¹¶æ‰“å°
        os.system('cls' if os.name == 'nt' else 'clear')
        
        print("=" * 70)
        print("ğŸš€ Vibe Coding æ·±åº¦åˆ†æå™¨ - DeepSeek API")
        print("=" * 70)
        print(f"ğŸ“Š è¿›åº¦: {s['success'] + s['failed']}/{total_to_process} ({progress:.1f}%)")
        print(f"âœ… æˆåŠŸ: {s['success']} | âŒ å¤±è´¥: {s['failed']} | â­ï¸ è·³è¿‡: {s['skipped']}")
        print(f"âš¡ é€Ÿåº¦: {rate:.1f} ä¸ª/åˆ†é’Ÿ | â±ï¸ é¢„è®¡å‰©ä½™: {eta_minutes} åˆ†é’Ÿ")
        print("-" * 70)
        print(f"ğŸ’° æˆæœ¬ç»Ÿè®¡:")
        print(f"   Token è¾“å…¥: {s['tokens_input']:,} | è¾“å‡º: {s['tokens_output']:,}")
        print(f"   æ€»æˆæœ¬: Â¥{s['total_cost_cny']:.4f} / é¢„ç®—: Â¥{MAX_BUDGET_CNY}")
        print("-" * 70)
        print("ğŸ“ˆ åˆ†ç±»åˆ†å¸ƒ (å®è§‚ç±»åˆ«):")
        for cat, count in sorted(self.category_stats['macro_category'].items(), key=lambda x: -x[1]):
            pct = count / s['success'] * 100 if s['success'] > 0 else 0
            print(f"   {cat}: {count} ({pct:.1f}%)")
        print("-" * 70)
        print("ğŸ¯ AI ç”Ÿæˆåˆ†æ•°åˆ†å¸ƒ:")
        scores = self.category_stats['ai_generation_score']
        score_str = " | ".join([f"{i}â˜…: {scores[i]}" for i in range(1, 6)])
        print(f"   {score_str}")
        print("=" * 70)
    
    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†ï¼šä¼˜é›…é€€å‡º"""
        print("\n\nğŸ›‘ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜è¿›åº¦...")
        self.running = False
        self.save_progress()
        print("âœ… è¿›åº¦å·²ä¿å­˜ï¼Œå¯ä»¥å®‰å…¨é€€å‡º")
        sys.exit(0)
    
    def run(self) -> None:
        """ä¸»è¿è¡Œæµç¨‹"""
        # æ£€æŸ¥ API Key
        if not self.api_key:
            print("âŒ é”™è¯¯: æœªè®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
            print("   è¯·åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ : DEEPSEEK_API_KEY=your_key")
            return
        
        # æ³¨å†Œä¿¡å·å¤„ç†
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        # æç¤ºè¯æ¥æºä¿¡æ¯
        prompt_source = f"ğŸ“„ {PROMPT_FILE}" if os.path.exists(PROMPT_FILE) else "âš ï¸ å†…ç½®æç¤ºè¯(æ–‡ä»¶ä¸å­˜åœ¨)"
        
        print("=" * 70)
        print("ğŸš€ Vibe Coding æ·±åº¦åˆ†æå™¨ - DeepSeek API")
        print("=" * 70)
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {INPUT_FILE}")
        print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {OUTPUT_JSON}, {OUTPUT_CSV}")
        print(f"ğŸ”§ å¹¶å‘æ•°: {MAX_WORKERS} | æ¨¡å‹: {DEEPSEEK_MODEL}")
        print(f"ğŸ’° é¢„ç®—ä¸Šé™: Â¥{MAX_BUDGET_CNY}")
        print(f"ğŸ“ æç¤ºè¯æ¥æº: {prompt_source}")
        print("=" * 70)
        
        # åŠ è½½è¿›åº¦å’Œæ•°æ®
        self.load_progress()
        repos = self.load_repos()
        
        if not repos:
            print("âœ… æ²¡æœ‰å¾…å¤„ç†çš„æ•°æ®")
            return
        
        # æ£€æŸ¥é¢„ç®—
        estimated_cost = len(repos) * 0.03  # æ¯ä¸ªçº¦ 3 åˆ†é’±
        print(f"ğŸ’¡ é¢„è®¡æ€»æˆæœ¬: Â¥{estimated_cost:.2f}")
        if estimated_cost > MAX_BUDGET_CNY:
            print(f"âš ï¸ è­¦å‘Š: é¢„è®¡æˆæœ¬è¶…è¿‡é¢„ç®—ï¼Œå»ºè®®è°ƒæ•´ MAX_BUDGET_CNY æˆ–å‡å°‘å¤„ç†æ•°é‡")
        
        input("\næŒ‰ Enter å¼€å§‹åˆ†æï¼Œæˆ– Ctrl+C é€€å‡º...")
        
        self.stats['start_time'] = time.time()
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_repo = {
                executor.submit(self.analyze_single, repo): repo 
                for repo in repos
            }
            
            # å¤„ç†ç»“æœ
            for future in as_completed(future_to_repo):
                if not self.running:
                    break
                
                repo = future_to_repo[future]
                self.stats['processed'] += 1
                
                try:
                    result = future.result()
                    if result:
                        self._save_result(result)
                        
                        # æ£€æŸ¥é¢„ç®—
                        if self.stats['total_cost_cny'] >= MAX_BUDGET_CNY:
                            print(f"\nğŸ’° å·²è¾¾åˆ°é¢„ç®—ä¸Šé™ Â¥{MAX_BUDGET_CNY}ï¼Œåœæ­¢å¤„ç†")
                            self.running = False
                            break
                    else:
                        self.stats['failed'] += 1
                        
                except Exception as e:
                    print(f"    âŒ å¤„ç†å¼‚å¸¸: {e}")
                    self.stats['failed'] += 1
                
                # æ¯ 10 ä¸ªæ›´æ–°ä¸€æ¬¡è¿›åº¦æ˜¾ç¤º
                if self.stats['processed'] % 10 == 0:
                    self._print_progress()
                    self.save_progress()
                
                # è¯·æ±‚é—´éš”
                time.sleep(REQUEST_DELAY)
        
        # å®Œæˆ
        self.stats['end_time'] = time.time()
        self.save_progress()
        self._print_final_stats()
    
    def _print_final_stats(self) -> None:
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡"""
        s = self.stats
        elapsed = s['end_time'] - s['start_time'] if s['end_time'] else 0
        
        print("\n" + "=" * 70)
        print("ğŸ‰ åˆ†æå®Œæˆ!")
        print("=" * 70)
        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"   æ€»è®¡: {s['success'] + s['failed']} | æˆåŠŸ: {s['success']} | å¤±è´¥: {s['failed']}")
        print(f"â±ï¸  ç”¨æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ | å¹³å‡: {s['success']/(elapsed/60):.1f} ä¸ª/åˆ†é’Ÿ")
        print(f"ğŸ’° æ€»æˆæœ¬: Â¥{s['total_cost_cny']:.4f}")
        print(f"ğŸ“ Token: è¾“å…¥ {s['tokens_input']:,} | è¾“å‡º {s['tokens_output']:,}")
        print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"   JSON: {OUTPUT_JSON}")
        print(f"   CSV: {OUTPUT_CSV}")
        if os.path.exists(FAILED_FILE):
            print(f"   å¤±è´¥è®°å½•: {FAILED_FILE}")
        print("=" * 70)


def main():
    analyzer = VibeCodingAnalyzer()
    try:
        analyzer.run()
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
        analyzer.save_progress()
    except Exception as e:
        print(f"\nâŒ è¿è¡Œæ—¶é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        analyzer.save_progress()


if __name__ == "__main__":
    main()
