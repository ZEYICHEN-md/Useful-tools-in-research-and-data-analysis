#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Vibe Coding åˆ†æç»“æœæ±‡æ€»ä¸å¯è§†åŒ–

åŠŸèƒ½:
- è¯»å–åˆ†æç»“æœï¼Œç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
- ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
- å¯¼å‡ºæ´å¯Ÿæ‘˜è¦
"""

import json
import csv
import os
from collections import Counter
from datetime import datetime


def load_results(jsonl_file: str = "vibe_coding_analysis.jsonl"):
    """åŠ è½½åˆ†æç»“æœ"""
    results = []
    if not os.path.exists(jsonl_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {jsonl_file}")
        return results
    
    with open(jsonl_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                results.append(json.loads(line))
            except json.JSONDecodeError:
                continue
    
    return results


def generate_summary(results):
    """ç”Ÿæˆç»Ÿè®¡æ‘˜è¦"""
    if not results:
        print("æ²¡æœ‰æ•°æ®")
        return
    
    total = len(results)
    
    # åŸºç¡€ç»Ÿè®¡
    macro_cats = Counter([r.get('macro_category', 'unknown') for r in results])
    micro_scenes = Counter([r.get('micro_scenario', 'unknown') for r in results])
    ai_scores = Counter([r.get('ai_generation_score', 0) for r in results])
    complexity = Counter([r.get('complexity_level', 0) for r in results])
    languages = Counter([r.get('language', 'Unknown') for r in results])
    tiers = Counter([r.get('tier', 'unknown') for r in results])
    
    # Stars ç»Ÿè®¡
    stars_list = [r.get('stars', 0) for r in results]
    avg_stars = sum(stars_list) / len(stars_list) if stars_list else 0
    
    # é«˜æ˜Ÿçº§é¡¹ç›® (>50 stars)
    high_star_repos = [r for r in results if r.get('stars', 0) > 50]
    
    # AI é«˜åˆ†é¡¹ç›® (>=4)
    high_ai_repos = [r for r in results if r.get('ai_generation_score', 0) >= 4]
    
    print("=" * 80)
    print("ğŸ“Š Vibe Coding åˆ†æç»“æœæ±‡æ€»")
    print("=" * 80)
    print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡ (æ ·æœ¬æ•°: {total})")
    print(f"   å¹³å‡ Stars: {avg_stars:.1f}")
    print(f"   é«˜æ˜Ÿé¡¹ç›® (>50â˜…): {len(high_star_repos)} ({len(high_star_repos)/total*100:.1f}%)")
    print(f"   é«˜ AI æµ“åº¦ (>=4â˜…): {len(high_ai_repos)} ({len(high_ai_repos)/total*100:.1f}%)")
    
    print(f"\nğŸ—ï¸ å®è§‚ç±»åˆ«åˆ†å¸ƒ:")
    for cat, count in macro_cats.most_common():
        pct = count / total * 100
        bar = "â–ˆ" * int(pct / 2)
        print(f"   {cat:20s}: {count:4d} ({pct:5.1f}%) {bar}")
    
    print(f"\nğŸ¯ å¾®è§‚åœºæ™¯åˆ†å¸ƒ (Top 10):")
    for scene, count in micro_scenes.most_common(10):
        pct = count / total * 100
        bar = "â–ˆ" * int(pct / 2)
        print(f"   {scene:20s}: {count:4d} ({pct:5.1f}%) {bar}")
    
    print(f"\nğŸ¤– AI ç”Ÿæˆåˆ†æ•°åˆ†å¸ƒ:")
    for score in range(1, 6):
        count = ai_scores.get(score, 0)
        pct = count / total * 100
        bar = "â–ˆ" * int(pct / 2)
        print(f"   {'â˜…' * score}{'â˜†' * (5-score)} ({score}): {count:4d} ({pct:5.1f}%) {bar}")
    
    print(f"\nğŸ“Š å¤æ‚åº¦åˆ†å¸ƒ:")
    for level in range(1, 6):
        count = complexity.get(level, 0)
        pct = count / total * 100
        bar = "â–ˆ" * int(pct / 2)
        labels = {1: "ç®€å•è„šæœ¬", 2: "è½»é‡å·¥å…·", 3: "ä¸­ç­‰åº”ç”¨", 4: "å¤æ‚ç³»ç»Ÿ", 5: "ä¼ä¸šçº§"}
        print(f"   {level} - {labels[level]:8s}: {count:4d} ({pct:5.1f}%) {bar}")
    
    print(f"\nğŸ’» ç¼–ç¨‹è¯­è¨€åˆ†å¸ƒ (Top 10):")
    for lang, count in languages.most_common(10):
        pct = count / total * 100
        bar = "â–ˆ" * int(pct / 2)
        print(f"   {lang or 'Unknown':15s}: {count:4d} ({pct:5.1f}%) {bar}")
    
    print(f"\nâ­ åˆ†å±‚åˆ†å¸ƒ:")
    for tier, count in tiers.most_common():
        pct = count / total * 100
        label = "æ²‰é»˜å¤§å¤šæ•°" if tier == "silent" else "é«˜ä»·å€¼ä¿¡å·"
        print(f"   {tier} ({label}): {count:4d} ({pct:5.1f}%)")
    
    # äº¤å‰åˆ†æ
    print(f"\nğŸ” å…³é”®æ´å¯Ÿ:")
    
    # AI æµ“åº¦ vs é¡¹ç›®ç±»å‹
    ai_by_macro = {}
    for r in results:
        macro = r.get('macro_category', 'unknown')
        ai = r.get('ai_generation_score', 0)
        if macro not in ai_by_macro:
            ai_by_macro[macro] = []
        ai_by_macro[macro].append(ai)
    
    print(f"\n   AI æµ“åº¦ vs é¡¹ç›®ç±»å‹ (å¹³å‡ AI åˆ†æ•°):")
    for macro, scores in sorted(ai_by_macro.items(), key=lambda x: -sum(x[1])/len(x[1])):
        avg = sum(scores) / len(scores)
        print(f"      {macro:20s}: {avg:.2f}/5.0")
    
    # çƒ­é—¨åœºæ™¯ç»„åˆ
    print(f"\n   çƒ­é—¨åœºæ™¯ç»„åˆ (Macro + Micro):")
    combos = Counter([(r.get('macro_category'), r.get('micro_scenario')) for r in results])
    for (macro, micro), count in combos.most_common(5):
        print(f"      {macro} + {micro}: {count} ä¸ªé¡¹ç›®")
    
    print("\n" + "=" * 80)
    
    # å¯¼å‡ºè¯¦ç»†æ´å¯Ÿ
    export_insights(results, high_star_repos, high_ai_repos)


def export_insights(results, high_star_repos, high_ai_repos):
    """å¯¼å‡ºæ´å¯Ÿæ‘˜è¦"""
    output_file = "analysis_insights.txt"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("ğŸŒŸ Vibe Coding èµ›é“æ·±åº¦æ´å¯Ÿ\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("ã€ä¸€ã€é«˜ä»·å€¼é¡¹ç›®æ¡ˆä¾‹ (>50 stars)ã€‘\n\n")
        for r in sorted(high_star_repos, key=lambda x: -x.get('stars', 0))[:10]:
            f.write(f"  ğŸ“Œ {r.get('repo_name')}\n")
            f.write(f"     â­ {r.get('stars')} | AIæµ“åº¦: {r.get('ai_generation_score')}/5\n")
            f.write(f"     ğŸ“ {r.get('core_intent')}\n")
            f.write(f"     ğŸ”— {r.get('repo_url')}\n")
            f.write(f"     ğŸ’¡ æ´å¯Ÿ: {r.get('analytical_insight')}\n\n")
        
        f.write("\nã€äºŒã€å…¸å‹ AI Native é¡¹ç›® (AIæµ“åº¦ >=4)ã€‘\n\n")
        for r in sorted(high_ai_repos, key=lambda x: -x.get('ai_generation_score', 0))[:10]:
            f.write(f"  ğŸ¤– {r.get('repo_name')} (AIæµ“åº¦: {r.get('ai_generation_score')}/5)\n")
            f.write(f"     â­ {r.get('stars')} | {r.get('macro_category')}\n")
            f.write(f"     ğŸ“ {r.get('core_intent')}\n")
            f.write(f"     ğŸ’¡ {r.get('analytical_insight')}\n\n")
        
        # åœºæ™¯æ´å¯Ÿ
        f.write("\nã€ä¸‰ã€åœºæ™¯åˆ†å¸ƒæ´å¯Ÿã€‘\n\n")
        
        scenes = {}
        for r in results:
            scene = r.get('micro_scenario', 'other')
            if scene not in scenes:
                scenes[scene] = []
            scenes[scene].append(r)
        
        for scene, repos in sorted(scenes.items(), key=lambda x: -len(x[1]))[:5]:
            avg_ai = sum(r.get('ai_generation_score', 0) for r in repos) / len(repos)
            f.write(f"  ğŸ“ {scene} ({len(repos)} ä¸ªé¡¹ç›®)\n")
            f.write(f"     å¹³å‡ AI æµ“åº¦: {avg_ai:.2f}/5\n")
            # æ‰¾ä»£è¡¨æ€§é¡¹ç›®
            example = max(repos, key=lambda x: x.get('stars', 0))
            f.write(f"     ä»£è¡¨é¡¹ç›®: {example.get('repo_name')} - {example.get('core_intent')}\n\n")
    
    print(f"ğŸ“„ è¯¦ç»†æ´å¯Ÿå·²å¯¼å‡ºåˆ°: {output_file}")


def main():
    results = load_results()
    if results:
        generate_summary(results)
    else:
        print("è¯·ç¡®ä¿å·²è¿è¡Œ deepseek_analyzer.py ç”Ÿæˆåˆ†æç»“æœ")


if __name__ == "__main__":
    main()
